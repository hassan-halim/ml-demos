{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題: 飛行機の遅延を予測する\n",
    "\n",
    "このノートブックの各種目標は以下のとおりです。\n",
    "- ダウンロードした ZIP ファイルを処理してデータセットを作成する\n",
    "- 探索的データ分析 (EDA) を行う\n",
    "- ベースラインモデルを確立する\n",
    "- シンプルなモデルからアンサンブルモデルに移行する\n",
    "- ハイパーパラメータを最適化する\n",
    "- 特徴量の重要度を確認する\n",
    "\n",
    "## ビジネスシナリオの概要\n",
    "あなたは勤務先の会社で、旅行予約ウェブサイトの運営を担当しています。このウェブサイトでは、フライトが遅延した場合のカスタマーエクスペリエンスの向上に取り組んでいます。自社では、米国の国内線で発着便数の非常に多い空港を発着するフライトを予約している顧客向けに、天候によりフライトが遅延するかどうかを知らせる機能を作成したいと考えています。\n",
    "\n",
    "今回のタスクは、天候によりフライトが遅延するかどうかを機械学習で特定し、この問題の一端を解決することです。データセットは、大手航空会社が運航する国内線の定刻パフォーマンスに関するものを使用します。このデータを使用して機械学習モデルをトレーニングし、発着便数の非常に多い空港でフライトが遅延するかどうかを予測します。\n",
    "\n",
    "## このデータセットについて\n",
    "データセットには、予定離着陸時刻と実際の離着陸時刻が含まれます。これは、米国内の予定旅客輸送収益の 1% 以上を占める、米国の認可航空会社から報告されたものです。データは、米国運輸統計局 (BTS) の航空情報庁が収集しました。データセットは、2013 年から 2018 年のフライトの日付、飛行時間、出発地、目的地、航空会社、飛行距離、遅延ステータスで構成されます。\n",
    "\n",
    "### 特徴量\n",
    "データセットの特徴量の詳細については、[On-time delay dataset features](https://www.transtats.bts.gov/Fields.asp) を参照してください。\n",
    "\n",
    "### データセットの属性  \n",
    "ウェブサイト: https://www.transtats.bts.gov/\n",
    "\n",
    "このラボで使用するデータセットは、米国運輸統計局 (BTS) の航空情報庁でまとめられたものであり、航空機の定刻パフォーマンスデータは https://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&amp;DB_URL=Mode_ID=1&amp;Mode_Desc=Aviation&amp;Subject_ID2=0 で確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ステップ 1: 問題の定式化とデータ収集\n",
    "\n",
    "このプロジェクトを始めるにあたり、このシナリオにおけるビジネス上の問題と達成しようとしているビジネス目標を 2、3 文にまとめて、以下に入力します。これには、チームが目指す必要のあるビジネスメトリクスを含めます。その情報を定義したら、機械学習の問題文を明確に書き出します。最後に、これが表す機械学習のタイプに関するコメントを 1、2 文追加します。\n",
    "\n",
    "#### <span style=\"color: blue;\">プロジェクトプレゼンテーション: これらの詳細の要約をプロジェクトプレゼンテーションに含めます。</span>\n",
    "\n",
    "### 1.機械学習がデプロイすべき適切なソリューションかどうか、また適切なソリューションである場合はその理由を判断します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.ビジネス上の問題、成功のメトリクス、求める機械学習の出力を定式化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.取り組んでいる機械学習の問題のタイプを特定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.使用しているデータの適切性を分析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定\n",
    "\n",
    "注力する領域を特定したところで、問題を解決するための作業を開始できるようにセットアップしましょう。\n",
    "\n",
    "**注意:** このノートブックは `ml.m4.xlarge` ノートブックインスタンスで作成およびテストされました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib2 import Path\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ステップ 2: データの前処理と可視化  \n",
    "このデータの前処理フェーズでは、データを探索して可視化し、データに対する理解を深める必要があります。まず、必要なライブラリをインポートし、データを pandas の DataFrame に読み込みます。その後、データを探索します。データセットのシェイプを確認し、作業している列と列のタイプ (数値、カテゴリ) を調べます。特徴量に対して基本的な統計を実行して、特徴量の平均と範囲を理解することを検討します。ターゲット列を詳細に調べて、その分布を判断します。\n",
    "\n",
    "### 考慮すべき具体的な質問\n",
    "1.特徴量に対して実行した基本的な統計からどのようなことを推測できますか? \n",
    "\n",
    "2.ターゲットクラスの分布から、どのようなことを推測できますか?\n",
    "\n",
    "3.データを探索することで推測できたことは他にありますか?\n",
    "\n",
    "#### <span style=\"color: blue;\">プロジェクトプレゼンテーション: これらの質問や他の同様の質問に対する自分の回答の要約をプロジェクトプレゼンテーションに含めます。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、Amazon S3 パブリックバケットからこのノートブック環境にデータセットを取り込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the file is already in the desired path or if it needs to be downloaded\n",
    "\n",
    "base_path = '/home/ec2-user/SageMaker/project/data/FlightDelays/'\n",
    "csv_base_path = '/home/ec2-user/SageMaker/project/data/csvFlightDelays/'\n",
    "file_path = 'On_Time_Reporting_Carrier_On_Time_Performance_1987_present_2014_1.zip'\n",
    "\n",
    "if not os.path.isfile(base_path + file_path):\n",
    "    subprocess.run(['mkdir', '-p', base_path])\n",
    "    subprocess.run(['mkdir', '-p', csv_base_path])\n",
    "    subprocess.run(['aws', 's3', 'cp',\n",
    "                    's3://aws-tc-largeobjects/ILT-TF-200-MLDWTS/flight_delay_project/csvFlightData-5/',\n",
    "                    base_path,'--recursive'])\n",
    "else:\n",
    "    print('File already downloaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = [str(file) for file in list(Path(base_path).iterdir()) if '.zip' in str(file)]\n",
    "len(zip_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ZIP ファイルから CSV ファイルを抽出する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def zip2csv(zipFile_name , file_path = '/home/ec2-user/SageMaker/project/data/csvFlightDelays'):\n",
    "    \"\"\"\n",
    "    Extract csv from zip files\n",
    "    zipFile_name: name of the zip file\n",
    "    file_path : name of the folder to store csv\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with ZipFile(zipFile_name, 'r') as z: \n",
    "            print(f'Extracting {zipFile_name} ') \n",
    "            z.extractall(path=file_path) \n",
    "    except:\n",
    "        print(f'zip2csv failed for {zipFile_name}')\n",
    "\n",
    "for file in zip_files:\n",
    "    zip2csv(file)\n",
    "\n",
    "print(\"Files Extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [str(file) for file in list(Path(csv_base_path).iterdir()) if '.csv' in str(file)]\n",
    "len(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV ファイルをロードする前に、展開したフォルダから HTML ファイルを読み取ります。この HTML ファイルに、データセットに含まれる特徴量の背景情報と詳細情報が含まれています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"./data/csvFlightDelays/readme.html\", width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### サンプル CSV をロードする\n",
    "\n",
    "すべての CSV ファイルを結合する前に、CSV ファイルごとにデータを読み取ります。pandas を使って、まず `On_Time_Reporting_Carrier_On_Time_Performance_(1987_present)_2018_9.csv` を読み取ります。Python の組み込みの `read_csv` 関数 ([ドキュメント](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)) を使用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv(<CODE> # **ENTER YOUR CODE HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**問題**: データセット内の行と列の長さ、および列名を出力してください。\n",
    "\n",
    "**ヒント**: DataFrame の行と列の出力には `<dataframe>.shape` 関数を使用し、列名の出力には `<dataframe>.columns` 関数を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shape = # **ENTER YOUR CODE HERE**\n",
    "print(f'Rows and columns in one csv file is {df_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**問題**: データセットの最初の 10 行を出力してください。 \n",
    "\n",
    "**ヒント**: `x` 個の行の出力には pandas の組み込み関数 `head(x)` を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**問題**: データセット内の列をすべて出力してください。`<dataframe>.columns` を使用して列名を出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The column names are :')\n",
    "print('#########')\n",
    "for col in <CODE>:# **ENTER YOUR CODE HERE**\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**問題**: データセットから、'Del' という単語を含む列をすべて出力してください。この操作によって、遅延データがある列の数を確認できます。\n",
    "\n",
    "**ヒント**: Python のリストに関する理解に基づいて、`if` ステートメントに特定の条件を指定する値を含めることができます。\n",
    "\n",
    "例: `[x for x in [1,2,3,4,5] if x > 2]`  \n",
    "\n",
    "**ヒント**: `in` キーワード ([ドキュメント](https://www.w3schools.com/python/ref_keyword_in.asp)) を使用して、値がリストにあるかどうかを確認できます。\n",
    "\n",
    "例: `5 in [1,2,3,4,5]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの分析に役立つ質問を以下にいくつか紹介します。\n",
    "\n",
    "**問題**   \n",
    "1.データセットに行と列はそれぞれいくつありますか?   \n",
    "2.データセットに何年分のデータが含まれていますか?   \n",
    "3.データセットが対象とする期間はどのくらいですか?   \n",
    "4.データセットにどの航空会社が含まれていますか?   \n",
    "5.どの発着空港が対象となっていますか?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The #rows and #columns are \", <CODE> , \" and \", <CODE>)\n",
    "print(\"The years in this dataset are: \", <CODE>)\n",
    "print(\"The months covered in this dataset are: \", <CODE>)\n",
    "print(\"The date range for data is :\" , min(<CODE>), \" to \", max(<CODE>))\n",
    "print(\"The airlines covered in this dataset are: \", list(<CODE>))\n",
    "print(\"The Origin airports covered are: \", list(<CODE>))\n",
    "print(\"The Destination airports covered are: \", list(<CODE>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**問題**: 発着空港の総数はいくつですか?\n",
    "\n",
    "**ヒント**: pandas の `values_count` 関数 ([ドキュメント](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html)) を使って、列 `Origin` と `Dest` で各空港の値を確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame({'Origin':<CODE>, 'Destination':<CODE>})\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**問題**: データセット内のフライト数に基づいて上位 15 件の発着空港を出力してください。\n",
    "\n",
    "**ヒント**: pandas の `sort_values` 関数 ([ドキュメント](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html)) を使用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.sort_values(by=<CODE>,ascending=False).head(15 )# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**問題**: フライトに関するすべての情報に基づいて、そのフライトが遅延するかどうかを予測できますか?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば、出張でサンフランシスコからロサンゼルスへ向かうとします。ロサンゼルスでの予約を効率良く管理できるよう、一連の特徴量に基づき、フライトが遅延するかどうかを知りたいと考えています。このデータセットには、フライトの前に確認できる特徴量がいくつありますか?\n",
    "\n",
    "`DepDelay`、`ArrDelay`、`CarrierDelay`、`WeatherDelay`、`NASDelay`、`SecurityDelay`、`LateAircraftDelay`、`DivArrDelay` などの列に、遅延に関する情報が含まれています。ただし、この遅延は出発地と目的地のいずれかで発生しました。着陸 10 分前に天候の急変で遅延が発生した場合、このデータはロサンゼルスでの予約を管理するうえでは役立ちません。\n",
    "\n",
    "そこで、問題文をシンプルにするため、以下の列に基づいて到着時の遅延を予測します。<br>\n",
    "\n",
    "`Year`、`Quarter`、`Month`、`DayofMonth`、`DayOfWeek`、`FlightDate`、`Reporting_Airline`、`Origin`、`OriginState`、`Dest`、`DestState`、`CRSDepTime`、`DepDelayMinutes`、`DepartureDelayGroups`、`Cancelled`、`Diverted`、`Distance`、`DistanceGroup`、`ArrDelay`、`ArrDelayMinutes`、`ArrDel15`、`AirTime`\n",
    "\n",
    "また、以下のように発着空港をフィルタリングします。\n",
    "- 主要空港: ATL、ORD、DFW、DEN、CLT、LAX、IAH、PHX、SFO\n",
    "- トップ 5 の航空会社: UA、OO、WN、AA、DL\n",
    "\n",
    "この操作を行うと、結合する CSV ファイル全体のデータのサイズを縮小できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### すべての CSV ファイルを結合する\n",
    "\n",
    "**ヒント**:  \n",
    "まず、各ファイルから個々の DataFrame をコピーするために使用する空の DataFrame を作成します。次に、`csv_files` リスト内のファイルごとに以下の操作を行います。\n",
    "\n",
    "1.DataFrame に CSV ファイルを読み込みます。  \n",
    "2.`filter_cols` 変数に基づいて列をフィルタリングします。\n",
    "\n",
    "```\n",
    "        columns = ['col1', 'col2']\n",
    "        df_filter = df[columns]\n",
    "```\n",
    "\n",
    "3.各 subset_cols に subset_vals のみを残します。pandas の `isin` 関数 ([ドキュメント](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isin.html)) を使用して、`val` が DataFrame の列にあるかどうかを確認し、それを含む行を選択します。\n",
    "\n",
    "```\n",
    "        df_eg[df_eg['col1'].isin('5')]\n",
    "```\n",
    "\n",
    "4.DataFrame を空の DataFrame と連結します。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv(csv_files, filter_cols, subset_cols, subset_vals, file_name = 'data/combined_files.csv'):\n",
    "    \"\"\"\n",
    "    Combine csv files into one Data Frame\n",
    "    csv_files: list of csv file paths\n",
    "    filter_cols: list of columns to filter\n",
    "    subset_cols: list of columns to subset rows\n",
    "    subset_vals: list of list of values to subset rows\n",
    "    \"\"\"\n",
    "    # Create an empty dataframe\n",
    "    df = # Enter your code here \n",
    "    \n",
    "    for file in csv_files:\n",
    "        # Read the CSV file into a dataframe\n",
    "        df_temp = pd.read_csv(<CODE>)# Enter your code here\n",
    "        \n",
    "        # Filter the columns based on the filter_cols variable\n",
    "        # e.g. columns = ['col1', 'col2']\n",
    "        # df_filter = df[columns]\n",
    "        df_temp = # Enter your code here\n",
    "        \n",
    "        # Keep only the subset_vals in each of the subset_cols\n",
    "        # HINT: Use the `isin` function to check if the val is in dataframe column\n",
    "        # and then choose the rows that include it\n",
    "        # e.g. df[df['col1'].isin('5')]\n",
    "        for col, val in zip(subset_cols,subset_vals):\n",
    "            df_temp = # Enter your code here     \n",
    "        \n",
    "        # Use Pandas concatenate `pd.concat` to concatenate the main dataframe with the dataframe for each file\n",
    "        df = pd.concat([df, df_temp], axis=0)\n",
    "    \n",
    "        \n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f'Combined csv stored at {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols is the list of columns to predict Arrival Delay \n",
    "cols = ['Year','Quarter','Month','DayofMonth','DayOfWeek','FlightDate',\n",
    "        'Reporting_Airline','Origin','OriginState','Dest','DestState',\n",
    "        'CRSDepTime','Cancelled','Diverted','Distance','DistanceGroup',\n",
    "        'ArrDelay','ArrDelayMinutes','ArrDel15','AirTime']\n",
    "\n",
    "subset_cols = ['Origin', 'Dest', 'Reporting_Airline']\n",
    "\n",
    "# subset_vals is a list collection of the top origin and destination airports and top 5 airlines\n",
    "subset_vals = [['ATL', 'ORD', 'DFW', 'DEN', 'CLT', 'LAX', 'IAH', 'PHX', 'SFO'],\n",
    "               ['ATL', 'ORD', 'DFW', 'DEN', 'CLT', 'LAX', 'IAH', 'PHX', 'SFO'],\n",
    "               ['UA', 'OO', 'WN', 'AA', 'DL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の関数を使用して各種ファイルをすべて単一のファイルにマージして、簡単に読み取れるようにします。\n",
    "\n",
    "**注意**: この処理が完了するまで 5～7 分かかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "combine_csv(csv_files, cols, subset_cols, subset_vals)\n",
    "print(f'csv\\'s merged in {round((time.time() - start)/60,2)} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットをロードする\n",
    "\n",
    "結合されたデータセットをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(<CODE>)# Enter your code here to read the combined csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初のレコード 5 つを出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの分析に役立つ質問を以下にいくつか紹介します。\n",
    "\n",
    "**問題**   \n",
    "1.データセットに行と列はそれぞれいくつありますか?   \n",
    "2.データセットに何年分のデータが含まれていますか?   \n",
    "3.データセットが対象とする期間はどのくらいですか?   \n",
    "4.データセットにどの航空会社が含まれていますか?   \n",
    "5.どの発着空港が対象となっていますか?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The #rows and #columns are \", <CODE> , \" and \", <CODE>)\n",
    "print(\"The years in this dataset are: \", list(<CODE>))\n",
    "print(\"The months covered in this dataset are: \", sorted(list(<CODE>)))\n",
    "print(\"The date range for data is :\" , min(<CODE>), \" to \", max(<CODE>))\n",
    "print(\"The airlines covered in this dataset are: \", list(<CODE>))\n",
    "print(\"The Origin airports covered are: \", list(<CODE>))\n",
    "print(\"The Destination airports covered are: \", list(<CODE>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、**target column : is_delay** を定義しましょう (1 - 到着時刻の遅延が 15 分を超える場合、0 - それ以外の場合)。`rename` メソッドを使用して、列名を `ArrDel15` から `is_delay` に変更します。\n",
    "\n",
    "**ヒント**: pandas の `rename` 関数 ([ドキュメント](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html)) を使用できます。\n",
    "\n",
    "例:\n",
    "```\n",
    "df.rename(columns={'col1':'column1'}, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns=<CODE>, inplace=True) # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "列全体で null を検索します。`isnull()` 関数 ([ドキュメント](https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.isnull.html)) を使用できます。\n",
    "\n",
    "**ヒント**: `isnull()` では、特定の値が null かどうかが検出され、null の代わりにブール値 (True または False) が返されます。`sum(axis=0)` 関数を使用して列の数を合計します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1658130 行のうち、1.3% に当たる 22540 行に、到着時の遅延情報と飛行時間が含まれていません。これらの行を削除または補完できます。ドキュメントには、行の欠落については一切記載されていません。\n",
    "\n",
    "**ヒント**: `~` 演算子を使用して、`isnull()` の出力から null 以外の値を選択します。\n",
    "\n",
    "例:\n",
    "```\n",
    "null_eg = df_eg[~df_eg['column_name'].isnull()]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove null columns\n",
    "data = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRSDepTime から 24 時間形式の時刻を取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DepHourofDay'] = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **機械学習の問題文**\n",
    "- 一連の特徴量に基づいて、フライトの遅延が 15 分を超えるかどうかを予測できますか?\n",
    "- ターゲット変数に入る値は 0/1 のみのため、分類アルゴリズムを使用できます。\n",
    "\n",
    "モデリングに進む前に、特徴量の分散、相関などを必ず確認するようお勧めします。\n",
    "- これで、データの非線形/パターンがすべてわかる\n",
    "    - 線形モデル: 検出力/指数/相互作用の特徴量を追加する\n",
    "    - 非線形モデルを試す\n",
    "- データの不均衡 \n",
    "    - モデルパフォーマンスに偏りが生じないメトリクスを選択する (精度と AUC の比較)\n",
    "    - 加重/カスタム損失関数を使用する\n",
    "- データの欠落\n",
    "    - 単純な統計に基づいて補完を行う - 平均値、中央値、最頻値 (数値変数)、頻度クラス (カテゴリ変数)\n",
    "    - クラスターベースの補完 (KNN で列の値を予測)\n",
    "    - 列を削除する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ探索\n",
    "\n",
    "#### クラスでの遅延ありと遅延なしを比較する\n",
    "\n",
    "**ヒント**: `groupby` プロット ([ドキュメント](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)) と `bar` プロット ([ドキュメント](https://matplotlib.org/tutorials/introductory/pyplot.html)) を使用して、クラスの頻度と分散のグラフを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.groupby(<CODE>).size()/len(data) ).plot(kind='bar')# Enter your code here\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**問題**: 遅延ありと遅延なしの比率に関する棒グラフからどのようなことを推測できますか?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**問題**: \n",
    "\n",
    "- 遅延が最も多いのは何月ですか?\n",
    "- 遅延が最も多いのは何時ですか?\n",
    "- 遅延が最も多いのは何曜日ですか?\n",
    "- 遅延が最も多いのはどの航空会社ですか?\n",
    "- 遅延が最も多い発着空港はどこですか?\n",
    "- 飛行距離は遅延の要因ですか?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_columns = ['Month', 'DepHourofDay', 'DayOfWeek', 'Reporting_Airline', 'Origin', 'Dest']\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20,20), squeeze=False)\n",
    "# fig.autofmt_xdate(rotation=90)\n",
    "\n",
    "for idx, column in enumerate(viz_columns):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    temp = data.groupby(column)['is_delay'].value_counts(normalize=True).rename('percentage').\\\n",
    "    mul(100).reset_index().sort_values(column)\n",
    "    sns.barplot(x=column, y=\"percentage\", hue=\"is_delay\", data=temp, ax=ax)\n",
    "    plt.ylabel('% delay/no-delay')\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot( x=\"is_delay\", y=\"Distance\", data=data, fit_reg=False, hue='is_delay', legend=False)\n",
    "plt.legend(loc='center')\n",
    "plt.xlabel('is_delay')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量\n",
    "\n",
    "列と、列の具体的なタイプをすべて確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必須の列を以下のようにフィルタリングします。\n",
    "- 日付を表す列には Year、Quarter、Month、DayofMonth、DayOfWeek があるため、日付は冗長です。\n",
    "- OriginState と DestState の代わりに Origin コードと Dest コードを使用します。\n",
    "- フライトが遅延するかどうかを分類するだけであるため、TotalDelayMinutes、DepDelayMinutes、ArrDelayMinutes は必要ありません。\n",
    "\n",
    "DepHourofDay はターゲットとの定量的関係がないため、カテゴリ変数として扱います。\n",
    "- ワンホットエンコーディングを行う必要がある場合は、23 列増えます。\n",
    "- カテゴリ変数を扱うその他の方法として、ハッシュエンコーディング、平均値正規化エンコーディング、値のバケット化などがあります。\n",
    "- ここでは単にバケットに分割します。\n",
    "\n",
    "**ヒント**: 列タイプをカテゴリに変更するには、`astype` 関数 ([ドキュメント](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html)) を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = data.copy()\n",
    "data = data[[ 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n",
    "       'Reporting_Airline', 'Origin', 'Dest','Distance','DepHourofDay', 'is_delay']]\n",
    "categorical_columns = ['Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n",
    "       'Reporting_Airline', 'Origin', 'Dest', 'DepHourofDay', 'is_delay']\n",
    "for c in categorical_columns:\n",
    "    data[c] = data[c].astype('category')# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ワンホットエンコーディングを使用するには、上記で選択したカテゴリ列に pandas の `get_dummies` 関数を使用します。次に、pandas の `concat` 関数を使用して、生成した特徴量を連結して元のデータセットに戻すことができます。カテゴリ変数のエンコーディングには、キーワード `drop_first=True` を使用して**ダミーエンコーディング**を使うこともできます。ダミーエンコーディングの詳細については、https://en.wikiversity.org/wiki/Dummy_variable_(statistics) を参照してください。\n",
    "\n",
    "例:\n",
    "```\n",
    "pd.get_dummies(df[['column1','columns2']], drop_first=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies = pd.get_dummies(<CODE>, drop_first=True) # Enter your code here\n",
    "data = pd.concat([<CODE>, <CODE>], axis = 1)\n",
    "categorical_columns.remove('is_delay')\n",
    "data.drop(categorical_columns,axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの長さと新しい列を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答例:** \n",
    "```\n",
    "Index(['Distance', 'is_delay', 'Quarter_2', 'Quarter_3', 'Quarter_4',\n",
    "       'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6', 'Month_7',\n",
    "       'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12',\n",
    "       'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5',\n",
    "       'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9',\n",
    "       'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13',\n",
    "       'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17',\n",
    "       'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21',\n",
    "       'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25',\n",
    "       'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29',\n",
    "       'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3',\n",
    "       'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7',\n",
    "       'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA',\n",
    "       'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW',\n",
    "       'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO',\n",
    "       'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD',\n",
    "       'Dest_PHX', 'Dest_SFO'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、モデルトレーニングを行う準備ができました。データを分割する前に、列の名前を `is_delay` から `target` に変更します。\n",
    "\n",
    "**ヒント**: pandas の `rename` 関数 ([ドキュメント](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html)) を使用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {<CODE>:<CODE>}, inplace=True )# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">ラボ 2 の終わり</span>\n",
    "\n",
    "ローカルコンピュータにプロジェクトファイルを保存します。次の一連のステップを実行します。\n",
    "\n",
    "1.ページ上部にある [**File**] メニューをクリックします。\n",
    "\n",
    "1.[**Download as**] を選択し、[**Notebook(.ipynb)**] をクリックします。 \n",
    "\n",
    "これにより、現在のノートブックがコンピュータのデフォルトのダウンロードフォルダにダウンロードされます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ステップ 3: モデルのトレーニングと評価\n",
    "\n",
    "データセットを DataFrame から機械学習アルゴリズムで使用できる形式に変換するときに、実行する必要がある準備手順があります。Amazon SageMaker の場合、以下の手順を実行する必要があります。\n",
    "\n",
    "1.`sklearn.model_selection.train_test_split` を使用し、データを `train_data`、`validation_data`、`test_data` に分割します。 \n",
    "2.Amazon SageMaker トレーニングジョブで使用できる適切なファイル形式にデータセットを変換します。これは CSV ファイルまたは record protobuf のいずれかです。詳細については、[トレーニングの共通データ形式](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html) を参照してください。 \n",
    "3.Amazon S3 バケットにデータをアップロードします。バケットを作成したことがない場合は、[バケットの作成](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html) を参照してください。 \n",
    "\n",
    "以下のセルを使用して、これらの手順を完了します。必要に応じてセルを挿入および削除します。\n",
    "\n",
    "#### <span style=\"color: blue;\">プロジェクトプレゼンテーション: プロジェクトプレゼンテーションに、このフェーズでの主な決定事項を記録します。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニングとテストの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_training_sets(data):\n",
    "    \"\"\"\n",
    "    Convert data frame to train, validation and test\n",
    "    params:\n",
    "        data: The dataframe with the dataset to be split\n",
    "    Returns:\n",
    "        train_features: Training feature dataset\n",
    "        test_features: Test feature dataset \n",
    "        train_labels: Labels for the training dataset\n",
    "        test_labels: Labels for the test dataset\n",
    "        val_features: Validation feature dataset\n",
    "        val_labels: Labels for the validation dataset\n",
    "    \"\"\"\n",
    "    # Extract the target variable from the dataframe and convert the type to float32\n",
    "    ys = np.array(<CODE>).astype(\"float32\") # Enter your code here\n",
    "    \n",
    "    # Drop all the unwanted columns including the target column\n",
    "    drop_list = # Enter your code here\n",
    "    \n",
    "    # Drop the columns from the drop_list and convert the data into a NumPy array of type float32\n",
    "    xs = np.array(data.drop(<CODE>, axis=1)).astype(\"float32\")# Enter your code here\n",
    "    \n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Use the sklearn function train_test_split to split the dataset in the ratio train 80% and test 20%\n",
    "    # Example: train_test_split(x, y, test_size=0.3)\n",
    "    train_features, test_features, train_labels, test_labels = # Enter your code here\n",
    "    \n",
    "    # Use the sklearn function again to split the test dataset into 50% validation and 50% test\n",
    "    val_features, test_features, val_labels, test_labels = # Enter your code here\n",
    "    \n",
    "    \n",
    "    return train_features, test_features, train_labels, test_labels, val_features, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to create your datasets\n",
    "train_features, test_features, train_labels, test_labels, val_features, val_labels = create_training_sets(data)\n",
    "\n",
    "print(f\"Length of train_features is: {<CODE>}\")\n",
    "print(f\"Length of train_labels is: {<CODE>}\")\n",
    "print(f\"Length of val_features is: {<CODE>}\")\n",
    "print(f\"Length of val_labels is: {<CODE>}\")\n",
    "print(f\"Length of test_features is: {<CODE>}\")\n",
    "print(f\"Length of test_labels is: {<CODE>}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答例**\n",
    "```\n",
    "Length of train_features is: (1308472, 71)\n",
    "Length of train_labels is: (1308472,)\n",
    "Length of val_features is: (163559, 71)\n",
    "Length of val_labels is: (163559,)\n",
    "Length of test_features is: (163559, 71)\n",
    "Length of test_labels is: (163559,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベースライン分類モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.predictor import CSVSerializer\n",
    "from sagemaker.amazon.amazon_estimator import RecordSet\n",
    "import boto3\n",
    "\n",
    "num_classes = # Enter your code here\n",
    "\n",
    "# Instantiate the LinearLearner estimator object with 1 ml.m4.xlarge\n",
    "classifier_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               instance_count=<CODE>,\n",
    "                                               instance_type=<CODE>,\n",
    "                                               predictor_type=<CODE>,\n",
    "                                              binary_classifier_model_selection_criteria=<CODE>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルコード\n",
    "```\n",
    "num_classes = len(pd.unique(train_labels))\n",
    "classifier_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                              instance_count=1,\n",
    "                                              instance_type='ml.m4.xlarge',\n",
    "                                              predictor_type='binary_classifier',\n",
    "                                             binary_classifier_model_selection_criteria = 'cross_entropy_loss')\n",
    "                                              \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形学習の場合、トレーニングデータは protobuf または CSV のコンテンツタイプで受け入れ、推論リクエストは protobuf、CSV、JSON のいずれかのコンテンツタイプで受け入れます。トレーニングデータは特徴量と正解ラベルで構成されるのに対し、推論リクエストのデータは特徴量のみで構成されます。\n",
    "\n",
    "本番パイプラインでは、データを Amazon SageMaker の protobuf 形式に変換し、Amazon S3 に保存することをお勧めします。ただし、すばやく開始できるように、AWS では、データセットがローカルメモリに収まるほど小さい場合には `record_set` という便利なメソッドを利用して、変換とアップロードを実行できます。このメソッドは、既にお使いのような NumPy 配列に対応しているため、ここではそれを使ってみましょう。`RecordSet` オブジェクトでは、データの一時的な Amazon S3 の場所を記録します。`estimator.record_set` 関数を使用して、トレーニング、検証、テストのレコードを作成します。次に、`estimator.fit` 関数を使用してトレーニングジョブを開始します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create train, val, test records\n",
    "train_records = classifier_estimator.record_set(<CODE>,<CODE>, channel='train')# Enter your code here\n",
    "val_records = classifier_estimator.record_set(<CODE>,<CODE>, channel='validation')# Enter your code here\n",
    "test_records = classifier_estimator.record_set(<CODE>,<CODE>, channel='test')# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "では、アップロードしたデータセットでモデルをトレーニングします。\n",
    "\n",
    "### サンプルコード\n",
    "```\n",
    "linear.fit([train_records,val_records,test_records])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit the classifier\n",
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの評価\n",
    "このセクションでは、トレーニング済みモデルを評価します。まず、`estimator.deploy` 関数で `initial_instance_count= 1` と `instance_type= 'ml.m4.xlarge'` を指定し、モデルを Amazon SageMaker にデプロイします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deloy an endpoint for batch prediction\n",
    "classifier_predictor = classifier_estimator.deploy(initial_instance_count=<CODE>,\n",
    "                                                   instance_type=<CODE>) # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エンドポイントが 'InService' になったら、テストセットでのモデルのパフォーマンスを評価します。`predict_batches` 関数を使用してテストセットでのメトリクスを予測します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def predict_batches(predictor, features, labels):\n",
    "    \"\"\"\n",
    "    Return evaluation results\n",
    "    predictor : Predictor object of model\n",
    "    features: Input features to model\n",
    "    label: Ground truth target values\n",
    "    \"\"\"\n",
    "    prediction_batches = [predictor.predict(batch) for batch in np.array_split(features, 100)]\n",
    "\n",
    "    # Parse protobuf responses to extract predicted labels\n",
    "    extract_label = lambda x: x.label['predicted_label'].float32_tensor.values\n",
    "    preds = np.concatenate([np.array([extract_label(x) for x in batch]) for batch in prediction_batches])\n",
    "    preds = preds.reshape((-1,))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = (preds == labels).sum() / labels.shape[0]\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    \n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    print(f'AUC : {auc}')\n",
    "    \n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(labels, preds, average = 'binary')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1_score: {f1_score}')\n",
    "    \n",
    "    confusion_matrix = pd.crosstab(index=labels, columns=np.round(preds), rownames=['True'], colnames=['predictions']).astype(int)\n",
    "    plt.figure(figsize = (5,5))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap=\"YlGnBu\").set_title('Confusion Matrix') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "混同行列を出力するには、テストデータセットで `predict_batches` 関数を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考慮すべき主な質問:\n",
    "1.テストセットでのモデルのパフォーマンスはトレーニングセットでのパフォーマンスと比べてどのように異なりますか? この比較からどのようなことを推測できますか? \n",
    "\n",
    "2.正解率、適合率、再現率などのメトリクスの結果に明らかな違いはありますか? ある場合、そのような違いが見られる理由は何だと思われますか? \n",
    "\n",
    "3.ビジネスの状況と目標を考えると、ここで考慮すべき最も重要なメトリクスはどれですか? それはなぜですか?\n",
    "\n",
    "4.最も重要だと考えるメトリクスの結果は、ビジネスの観点でのニーズを十分に満たすものですか? そうでない場合、次のイテレーション (次の特徴量エンジニアリングのセクション) の際に何を変更できると思いますか? \n",
    "\n",
    "以下のセルを使用して、これらの質問や他の質問に答えてください。必要に応じてセルを挿入および削除します。\n",
    "\n",
    "#### <span style=\"color: blue;\">プロジェクトプレゼンテーション: これらの質問やこのセクションで回答する他の同様の質問に対する回答をプロジェクトプレゼンテーションに記録します。主な詳細と決定事項をプロジェクトプレゼンテーションに記録します。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**問題**: 混同行列からどのようなことを要約できますか?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">ラボ 3 の終わり</span>\n",
    "\n",
    "ローカルコンピュータにプロジェクトファイルを保存します。次の一連のステップを実行します。\n",
    "\n",
    "1.ページ上部にある [**File**] メニューをクリックします。\n",
    "\n",
    "1.[**Download as**] を選択し、[**Notebook(.ipynb)**] をクリックします。 \n",
    "\n",
    "これにより、現在のノートブックがコンピュータのデフォルトのダウンロードフォルダにダウンロードされます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# イテレーション 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ステップ 4: 特徴量エンジニアリング\n",
    "\n",
    "これで、モデルのトレーニングと評価の 1 回目のイテレーションを完了しました。モデルで最初に得られた結果はビジネス上の問題を解決するにはおそらく不十分だったと仮定すると、モデルのパフォーマンスを改善できるようにするためにデータに関して何を変更できると思いますか?\n",
    "\n",
    "### 考慮すべき主な質問:\n",
    "1.2 つのメインクラス (遅延ありと遅延なし) の均衡はモデルのパフォーマンスにどう影響する可能性がありますか?\n",
    "2.相関する特徴量はありますか?\n",
    "3.この段階で、モデルのパフォーマンスに良い影響を与える可能性があり、実行できる特徴量削減の手法はありますか? \n",
    "4.データ/データセットの追加を検討できますか?\n",
    "4.特徴量エンジニアリングを実行した結果、1 回目のイテレーションと比べてモデルのパフォーマンスはどうなりますか?\n",
    "\n",
    "以下のセルを使用し、上記の質問に従って、モデルのパフォーマンスが改善すると考えられる特徴量エンジニアリングの手法を実行します。必要に応じてセルを挿入および削除します。\n",
    "\n",
    "#### <span style=\"color: blue;\">プロジェクトプレゼンテーション: 主な決定事項とこのセクションで使用する手法をプロジェクトプレゼンテーションに記録します。また、モデルを再び評価した後に取得する新しいパフォーマンスメトリクスも記録します。</span>\n",
    "\n",
    "開始する前に、適合率と再現率が約 80% であるのに対し、精度が 99% である理由を考えてみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特徴量を追加する\n",
    "\n",
    "1.祝日\n",
    "2.天候"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2014 年から 2018 年の祝日はすべてわかっているため、指標変数 **is_holiday** を作成して祝日をマークできます。\n",
    "例えば、祝日期間中は他の日に比べてフライトの遅延率が高い可能性があるとします。2014～2018 年の祝日を含むブール変数 `is_holiday` を追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: http://www.calendarpedia.com/holidays/federal-holidays-2014.html\n",
    "\n",
    "holidays_14 = ['2014-01-01', '2014-01-20', '2014-02-17', '2014-05-26', '2014-07-04', '2014-09-01', '2014-10-13', '2014-11-11', '2014-11-27', '2014-12-25' ] \n",
    "holidays_15 = ['2015-01-01', '2015-01-19', '2015-02-16', '2015-05-25', '2015-06-03', '2015-07-04', '2015-09-07', '2015-10-12', '2015-11-11', '2015-11-26', '2015-12-25'] \n",
    "holidays_16 = ['2016-01-01', '2016-01-18', '2016-02-15', '2016-05-30', '2016-07-04', '2016-09-05', '2016-10-10', '2016-11-11', '2016-11-24', '2016-12-25', '2016-12-26']\n",
    "holidays_17 = ['2017-01-02', '2017-01-16', '2017-02-20', '2017-05-29' , '2017-07-04', '2017-09-04' ,'2017-10-09', '2017-11-10', '2017-11-23', '2017-12-25']\n",
    "holidays_18 = ['2018-01-01', '2018-01-15', '2018-02-19', '2018-05-28' , '2018-07-04', '2018-09-03' ,'2018-10-08', '2018-11-12','2018-11-22', '2018-12-25']\n",
    "# holidays_19 = ['2019-01-01', '2019-01-21', '2019-02-18', '2019-05-27' , '2019-07-04', '2019-09-02' ,'2019-10-14', '2019-11-11','2019-11-28', '2019-12-25']\n",
    "holidays = holidays_14+ holidays_15+ holidays_16 + holidays_17+ holidays_18\n",
    "\n",
    "### Add indicator variable for holidays\n",
    "data_orig['is_holiday'] = # Enter your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "気象データは https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries&amp;stations=USW00023174,USW00012960,USW00003017,USW00094846,USW00013874,USW00023234,USW00003927,USW00023183,USW00013881&amp;dataTypes=AWND,PRCP,SNOW,SNWD,TAVG,TMIN,TMAX&amp;startDate=2014-01-01&amp;endDate=2018-12-31 から取得しました。\n",
    "<br>\n",
    "\n",
    "このデータセットには、都市の風速、降雨量、積雪、気温に関する情報が空港コード別に含まれています。\n",
    "\n",
    "**問題**: 降雨、強風、積雪による悪天候がフライトの遅延につながる可能性がありますか? 確認しましょう!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "aws s3 cp s3://aws-tc-largeobjects/ILT-TF-200-MLDWTS/flight_delay_project/daily-summaries.csv /home/ec2-user/SageMaker/project/data/\n",
    "#wget 'https://www.ncei.noaa.gov/access/services/data/v1?dataset=daily-summaries&amp;stations=USW00023174,USW00012960,USW00003017,USW00094846,USW00013874,USW00023234,USW00003927,USW00023183,USW00013881&amp;dataTypes=AWND,PRCP,SNOW,SNWD,TAVG,TMIN,TMAX&amp;startDate=2014-01-01&amp;endDate=2018-12-31' -O /home/ec2-user/SageMaker/project/data/daily-summaries.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "空港コードに合わせて作成された気象データをデータセットにインポートします。下記の測候所と空港を分析に使用し、測候所を空港名にマップする `airport` という新しい列を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv(<CODE>) # Enter your code here to read 'daily-summaries.csv' file\n",
    "station = ['USW00023174','USW00012960','USW00003017','USW00094846',\n",
    "           'USW00013874','USW00023234','USW00003927','USW00023183','USW00013881'] \n",
    "airports = ['LAX', 'IAH', 'DEN', 'ORD', 'ATL', 'SFO', 'DFW', 'PHX', 'CLT']\n",
    "\n",
    "### Map weather stations to airport code\n",
    "station_map = # Enter your code here \n",
    "weather['airport'] = # Enter your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DATE` 列から `MONTH` というもう 1 つの列を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather['MONTH'] = weather[<CODE>].apply(lambda x: x.split('-')[1])# Enter your code here \n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル出力\n",
    "```\n",
    "  STATION DATE AWND PRCP SNOW SNWD TAVG TMAX TMIN airport MONTH\n",
    "0 USW00023174 2014-01-01 16 0 NaN NaN 131.0 178.0 78.0 LAX 01\n",
    "1 USW00023174 2014-01-02 22 0 NaN NaN 159.0 256.0 100.0 LAX 01\n",
    "2 USW00023174 2014-01-03 17 0 NaN NaN 140.0 178.0 83.0 LAX 01\n",
    "3 USW00023174 2014-01-04 18 0 NaN NaN 136.0 183.0 100.0 LAX 01\n",
    "4 USW00023174 2014-01-05 18 0 NaN NaN 151.0 244.0 83.0 LAX 01\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SNOW` 列と `SNWD` 列を分析し、`fillna()` を使用して欠損値を処理します。`isna()` 関数を使用してすべての列で欠損値があるかどうかを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather.SNOW.fillna(<CODE>, inplace=True)# Enter your code here\n",
    "weather.SNWD.fillna(<CODE>, inplace=True)# Enter your code here\n",
    "weather.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**問題**: TAVG、TMAX、TMIN に欠損値がある行のインデックスを出力できますか?\n",
    "\n",
    "**ヒント**: `isna()` 関数を使用して欠落している行を探し、idx 変数でリストを使用してインデックスを取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array([i for i in range(len(weather))])\n",
    "TAVG_idx = # Enter your code here \n",
    "TMAX_idx = # Enter your code here \n",
    "TMIN_idx = # Enter your code here \n",
    "TAVG_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル出力\n",
    "\n",
    "```\n",
    "array([ 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964,\n",
    "        3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973,\n",
    "        3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982,\n",
    "        3983, 3984, 3985, 4017, 4018, 4019, 4020, 4021, 4022,\n",
    "        4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031,\n",
    "        4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040,\n",
    "        4041, 4042, 4043, 4044, 4045, 4046, 4047, 13420])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠落している TAVG、TMAX、TMIN に特定の測候所/空港の平均値を代入できます。TAVG_idx で連続する行が欠落しているため、直前の値を代入することはできません。代わりに、平均値を代入します。`groupby` 関数を使用して、平均値を含む変数を集計します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_impute = weather.groupby([<CODE>]).agg({'TAVG':'mean','TMAX':'mean', 'TMIN':'mean' }).reset_index()# Enter your code here\n",
    "weather_impute.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均値データを気象データとマージします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the yesterday's data\n",
    "weather = pd.merge(weather, weather_impute, how='left', left_on=['MONTH','STATION'], right_on = ['MONTH','STATION'])\\\n",
    ".rename(columns = {'TAVG_y':'TAVG_AVG',\n",
    "                   'TMAX_y':'TMAX_AVG',\n",
    "                   'TMIN_y':'TMIN_AVG',\n",
    "                   'TAVG_x':'TAVG',\n",
    "                   'TMAX_x':'TMAX',\n",
    "                   'TMIN_x':'TMIN'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠損値があるかどうかをもう一度確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.TAVG[TAVG_idx] = weather.TAVG_AVG[TAVG_idx]\n",
    "weather.TMAX[TMAX_idx] = weather.TMAX_AVG[TMAX_idx]\n",
    "weather.TMIN[TMIN_idx] = weather.TMIN_AVG[TMIN_idx]\n",
    "weather.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットから `STATION、MONTH、TAVG_AVG、TMAX_AVG、TMIN_AVG、TMAX、TMIN、SNWD` を削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.drop(columns=['STATION','MONTH','TAVG_AVG', 'TMAX_AVG', 'TMIN_AVG', 'TMAX' ,'TMIN', 'SNWD'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットに出発地と目的地の気象条件を追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add origin weather conditions\n",
    "data_orig = pd.merge(data_orig, weather, how='left', left_on=['FlightDate','Origin'], right_on = ['DATE','airport'])\\\n",
    ".rename(columns = {'AWND':'AWND_O','PRCP':'PRCP_O', 'TAVG':'TAVG_O', 'SNOW': 'SNOW_O'})\\\n",
    ".drop(columns=['DATE','airport'])\n",
    "\n",
    "### Add destination weather conditions\n",
    "data_orig = pd.merge(data_orig, weather, how='left', left_on=['FlightDate','Dest'], right_on = ['DATE','airport'])\\\n",
    ".rename(columns = {'AWND':'AWND_D','PRCP':'PRCP_D', 'TAVG':'TAVG_D', 'SNOW': 'SNOW_D'})\\\n",
    ".drop(columns=['DATE','airport'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**: 結合後に null/NA を確認することをお勧めします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ワンホットエンコーディングを使用してカテゴリデータを数値データに変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_orig.copy()\n",
    "data = data[['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n",
    "       'Reporting_Airline', 'Origin', 'Dest','Distance','DepHourofDay', 'is_delay', 'is_holiday', 'AWND_O', 'PRCP_O',\n",
    "       'TAVG_O', 'AWND_D', 'PRCP_D', 'TAVG_D', 'SNOW_O', 'SNOW_D']]\n",
    "\n",
    "\n",
    "categorical_columns = ['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n",
    "       'Reporting_Airline', 'Origin', 'Dest', 'is_holiday','is_delay']\n",
    "for c in categorical_columns:\n",
    "    data[c] = data[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルコード\n",
    "\n",
    "```\n",
    "data_dummies = pd.get_dummies(data[['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'Reporting_Airline', 'Origin', 'Dest', 'is_holiday']], drop_first=True)\n",
    "data = pd.concat([data, data_dummies], axis = 1)\n",
    "categorical_columns.remove('is_delay')\n",
    "data.drop(categorical_columns,axis=1, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新しい列を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル出力\n",
    "\n",
    "```\n",
    "Index(['Distance', 'DepHourofDay', 'is_delay', 'AWND_O', 'PRCP_O', 'TAVG_O',\n",
    "       'AWND_D', 'PRCP_D', 'TAVG_D', 'SNOW_O', 'SNOW_D', 'Year_2015',\n",
    "       'Year_2016', 'Year_2017', 'Year_2018', 'Quarter_2', 'Quarter_3',\n",
    "       'Quarter_4', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6',\n",
    "       'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12',\n",
    "       'DayofMonth_2', 'DayofMonth_3', 'DayofMonth_4', 'DayofMonth_5',\n",
    "       'DayofMonth_6', 'DayofMonth_7', 'DayofMonth_8', 'DayofMonth_9',\n",
    "       'DayofMonth_10', 'DayofMonth_11', 'DayofMonth_12', 'DayofMonth_13',\n",
    "       'DayofMonth_14', 'DayofMonth_15', 'DayofMonth_16', 'DayofMonth_17',\n",
    "       'DayofMonth_18', 'DayofMonth_19', 'DayofMonth_20', 'DayofMonth_21',\n",
    "       'DayofMonth_22', 'DayofMonth_23', 'DayofMonth_24', 'DayofMonth_25',\n",
    "       'DayofMonth_26', 'DayofMonth_27', 'DayofMonth_28', 'DayofMonth_29',\n",
    "       'DayofMonth_30', 'DayofMonth_31', 'DayOfWeek_2', 'DayOfWeek_3',\n",
    "       'DayOfWeek_4', 'DayOfWeek_5', 'DayOfWeek_6', 'DayOfWeek_7',\n",
    "       'Reporting_Airline_DL', 'Reporting_Airline_OO', 'Reporting_Airline_UA',\n",
    "       'Reporting_Airline_WN', 'Origin_CLT', 'Origin_DEN', 'Origin_DFW',\n",
    "       'Origin_IAH', 'Origin_LAX', 'Origin_ORD', 'Origin_PHX', 'Origin_SFO',\n",
    "       'Dest_CLT', 'Dest_DEN', 'Dest_DFW', 'Dest_IAH', 'Dest_LAX', 'Dest_ORD',\n",
    "       'Dest_PHX', 'Dest_SFO', 'is_holiday_1'],\n",
    "      dtype='object')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "列名を `is_delay` から `target` に戻します。前と同じコードを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {<CODE>:<CODE>}, inplace=True )# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニングセットをもう一度作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新しいベースライン分類器\n",
    "\n",
    "ここで、上記の新しい特徴量によりモデルの予測検出力が強化されたかどうかを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LinearLearner estimator object\n",
    "num_classes = # Enter your code here\n",
    "classifier_estimator = # Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルコード\n",
    "\n",
    "```\n",
    "num_classes = len(pd.unique(train_labels)) \n",
    "classifier_estimator = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                               instance_count=1,\n",
    "                                               instance_type='ml.m4.xlarge',\n",
    "                                               predictor_type='binary_classifier',\n",
    "                                              binary_classifier_model_selection_criteria = 'cross_entropy_loss')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = classifier_estimator.record_set(train_features, train_labels, channel='train')\n",
    "val_records = classifier_estimator.record_set(val_features, val_labels, channel='validation')\n",
    "test_records = classifier_estimator.record_set(test_features, test_labels, channel='test')\n",
    "\n",
    "classifier_estimator.fit([train_records, val_records, test_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_predictor = # Enter your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batches(classifier_predictor, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形モデルから、パフォーマンスが少ししか改善されなかったことがわかります。Amazon SageMaker で XGBoost というツリーベースのアンサンブルモデルを試します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost モデルを試す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の手順を行う必要があります。  \n",
    "\n",
    "1.トレーニングセット変数を使用します。それらの変数を CSV ファイル: train.csv、validation.csv、test.csv として保存します。  \n",
    "2.変数にバケット名を格納します。Amazon S3 バケット名は、ラボ手順の左側に表示されます。 \n",
    "a. `bucket = <LabBucketName>`  \n",
    "b. `prefix = 'sagemaker/xgboost'`  \n",
    "3.Boto3 を使用してバケットにモデルをアップロードします。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(data.sample(frac=1, random_state=1729), [int(0.7 * len(data)), int(0.9*len(data))])  \n",
    "\n",
    "pd.concat([train_data['target'], train_data.drop(['target'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['target'], validation_data.drop(['target'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)\n",
    "pd.concat([test_data['target'], test_data.drop(['target'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`<LabBucketName>`** をラボアカウントに用意されたリソース名に置き換えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here to define bucket and prefix\n",
    "bucket = <LabBucketName> # Enter your code here\n",
    "prefix = 'sagemaker/xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Upload data to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import image_uris\n",
    "container = image_uris.retrieve('xgboost',boto3.Session().region_name, '0.90-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sagemaker.s3_input` 関数を使用して、トレーニングと検証のデータセット用の record_set を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role = sagemaker.get_execution_role(),\n",
    "                                    instance_count=1,\n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        eval_metric = \"auc\",\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新しいモデルの予測器をデプロイし、テストデータセットでモデルを評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "xgb_predictor = xgb.deploy(initial_instance_count = <CODE>,\n",
    "                           instance_type = <CODE>,\n",
    "                          serializer=CSVSerializer())# Enter your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.concat([test_data['target'], test_data.drop(['target'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(predictor , features, labels , prob_threshold = 0.5, rows=500):\n",
    "    \"\"\"\n",
    "    Return evaluation results\n",
    "    predictor : Predictor object of model\n",
    "    features: Input features to model\n",
    "    label: Ground truth target values\n",
    "    prob_threshold : Probability cut off to separate positive and negative class\n",
    "    \"\"\"\n",
    "    split_array = np.array_split(features, int(features.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    preds = np.fromstring(predictions[1:], sep=',')\n",
    "    preds = preds.reshape((-1,))\n",
    "    predictions = np.where(preds > prob_threshold , 1, 0)\n",
    "    labels = labels.reshape((-1,))\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (predictions == labels).sum() / labels.shape[0]\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    \n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    print(f'AUC : {auc}')\n",
    "    \n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(labels, predictions, average = 'binary')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1_score: {f1_score}')\n",
    "    \n",
    "    \n",
    "    confusion_matrix = pd.crosstab(index=labels, columns=np.round(preds), rownames=['True'], colnames=['predictions']).astype(int)\n",
    "    plt.figure(figsize = (5,5))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap=\"YlGnBu\").set_title('Confusion Matrix') \n",
    "    return list(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(xgb_predictor, test_data.as_matrix()[:, 1:] , test_data.iloc[:, 0:1].as_matrix(), prob_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### さまざまなしきい値を試す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(xgb_predictor, test_data.as_matrix()[:, 1:] , test_data.iloc[:, 0:1].as_matrix(), prob_threshold = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**問題**: テストセットでのモデルのパフォーマンスからどのような結論を導くことができますか?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイパーパラメータの最適化 (HPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "### You can spin up multiple instances to do hyperparameter optimization in parallel\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role=sagemaker.get_execution_role(),\n",
    "                                    instance_count= 2, # make sure you have limit set for these instances\n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(eval_metric='auc',\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100,\n",
    "                        rate_drop=0.3,\n",
    "                        tweedie_variance_power=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 0.5),\n",
    "                        'min_child_weight': ContinuousParameter(3, 10),\n",
    "                         'num_round': IntegerParameter(10, 150),\n",
    "                        'alpha': ContinuousParameter(0, 2),\n",
    "                        'max_depth': IntegerParameter(10, 15)}\n",
    "\n",
    "objective_metric_name = 'validation:auc'\n",
    "\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i class=\"fas fa-exclamation-triangle\" style=\"color:red\"></i> トレーニングジョブが終了するまで待ちます。25～30 分ほどかかります。\n",
    "\n",
    "**ハイパーパラメータの最適化ジョブをモニタリングするには、以下を行います。**  \n",
    "\n",
    "1.AWS マネジメントコンソールの [**サービス**] メニューで [**Amazon SageMaker**] をクリックします。 \n",
    "1.[**Training**] > [**Hyperparameter tuning jobs**] の順にクリックします。 \n",
    "1.各ハイパーパラメータチューニングジョブのステータス、そのジョブの目標メトリクス値とログを確認できます。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sage_client = boto3.Session().client('sagemaker')\n",
    "tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "\n",
    "# Run this cell to check current status of hyperparameter tuning job\n",
    "tuning_job_result = sage_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)\n",
    "\n",
    "status = tuning_job_result['HyperParameterTuningJobStatus']\n",
    "if status != 'Completed':\n",
    "    print('Reminder: the tuning job has not been completed.')\n",
    "    \n",
    "job_count = tuning_job_result['TrainingJobStatusCounters']['Completed']\n",
    "print(\"%d training jobs have completed\" % job_count)\n",
    "    \n",
    "is_minimize = (tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['Type'] != 'Maximize')\n",
    "objective_name = tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['MetricName']\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "if tuning_job_result.get('BestTrainingJob',None):\n",
    "    print(\"Best model found so far:\")\n",
    "    pprint(tuning_job_result['BestTrainingJob'])\n",
    "else:\n",
    "    print(\"No training jobs have reported results yet.\")\n",
    "    \n",
    "### Get the hyperparameter tuning job results in a dataframe\n",
    "tuner_df = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name).dataframe()\n",
    "\n",
    "if len(tuner_df) > 0:\n",
    "    df = tuner_df[tuner_df['FinalObjectiveValue'] > -float('inf')]\n",
    "    if len(df) > 0:\n",
    "        df = df.sort_values('FinalObjectiveValue', ascending=is_minimize)\n",
    "        print(\"Number of training jobs with valid objective: %d\" % len(df))\n",
    "        print({\"lowest\":min(df['FinalObjectiveValue']),\"highest\": max(df['FinalObjectiveValue'])})\n",
    "        pd.set_option('display.max_colwidth', -1) # Don't truncate TrainingJobName        \n",
    "    else:\n",
    "        print(\"No training jobs have reported valid results yet.\")\n",
    "        \n",
    "tuner_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ハイパーパラメータの最適化トレーニングから得られた、精度が最も良いモデルをデプロイします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_hpo = # Enter your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(xgb_predictor_hpo, test_data.as_matrix()[:, 1:] , test_data.iloc[:, 0:1].as_matrix(), prob_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**問題**: 別のハイパーパラメータやハイパーパラメータ範囲を試してください。モデルが改善されますか?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量の重要度\n",
    "\n",
    "すべてのハイパーパラメータチューニングジョブのモデルファイルは、上記の表にあるトレーニングジョブ名で、フォルダ: \"{bucket}/{prefix}/output/\" に保存されます。モデルをロードし、通常の sklearn モデルオブジェクトのようにデータの解釈に使用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_hpo_model_path = \"s3://\" + bucket + \"/sagemaker/xgboost/output/<Best model TrainingJobName>/output/model.tar.gz\"\n",
    "best_hpo_model_path = <path-to-your-model>\n",
    "### Download best_hpo_model_path to local\n",
    "!aws s3 cp {best_hpo_model_path} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install xgboost\n",
    "!pip install xgboost=='0.90'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 選択したモデルファイルをロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tarfile\n",
    "import xgboost\n",
    "\n",
    "with open('model.tar.gz', 'rb') as f:\n",
    "    with tarfile.open(fileobj=f, mode='r') as tar_f:\n",
    "        with tar_f.extractfile('xgboost-model') as extracted_f:\n",
    "            xgbooster = pickle.load(extracted_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost モデルに列名をマップする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(data.columns)\n",
    "columns.remove('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = xgbooster.get_fscore()\n",
    "feature_importance_col = {}\n",
    "\n",
    "for column, fname in zip(columns, xgbooster.feature_names):\n",
    "    try:\n",
    "         feature_importance_col[column] = feature_importance[fname]\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特徴量の重要度値で並べ替える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(feature_importance_col.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプル出力\n",
    "\n",
    "```\n",
    "[('AWND_O', 13851),\n",
    " ('AWND_D', 13452),\n",
    " ('DepHourofDay', 13344),\n",
    " ('TAVG_O', 13106),\n",
    " ('TAVG_D', 12800),\n",
    " ('Distance', 8478),\n",
    " ('PRCP_O', 4210),\n",
    " ('PRCP_D', 3916),\n",
    " ('Reporting_Airline_UA', 1791),\n",
    " ('Year_2016', 1290),\n",
    " ('Year_2015', 1285),\n",
    " ('Year_2018', 1157),\n",
    " ('Quarter_4', 1092),\n",
    " ('Year_2017', 1009),\n",
    " ('DayOfWeek_5', 838),\n",
    " ('DayOfWeek_4', 833),\n",
    " ('Quarter_2', 799),\n",
    " ('DayOfWeek_7', 783),\n",
    " ('Reporting_Airline_WN', 736),\n",
    " ('DayOfWeek_3', 718),\n",
    " ('Origin_ORD', 706),\n",
    " ('DayOfWeek_2', 685),\n",
    " ('Reporting_Airline_DL', 663),\n",
    " ('Dest_LAX', 627),\n",
    " ('DayOfWeek_6', 614),\n",
    " ('Reporting_Airline_OO', 596),\n",
    " ('Origin_LAX', 590),\n",
    " ('Month_11', 565),\n",
    " ('Month_5', 543),\n",
    " ('Dest_ORD', 539),\n",
    " ('SNOW_O', 506),\n",
    " ('Month_8', 497),\n",
    " ('Month_12', 495),\n",
    " ('Month_7', 484),\n",
    " ('Origin_DFW', 480),\n",
    " ('Quarter_3', 477),\n",
    " ('Dest_DFW', 462),\n",
    " ('Origin_DEN', 456),\n",
    " ('Origin_SFO', 438),\n",
    " ('Month_6', 437),\n",
    " ('Dest_SFO', 437),\n",
    " ('Month_10', 422),\n",
    " ('Month_9', 407),\n",
    " ('Month_4', 394),\n",
    " ('Dest_DEN', 379),\n",
    " ('SNOW_D', 375),\n",
    " ('Month_3', 369),\n",
    " ('Dest_IAH', 369),\n",
    " ('Origin_PHX', 347),\n",
    " ('Dest_PHX', 346),\n",
    " ('Origin_IAH', 342),\n",
    " ('DayofMonth_15', 324),\n",
    " ('DayofMonth_17', 322),\n",
    " ('DayofMonth_21', 322),\n",
    " ('Origin_CLT', 322),\n",
    " ('DayofMonth_20', 316),\n",
    " ('DayofMonth_9', 313),\n",
    " ('DayofMonth_19', 307),\n",
    " ('DayofMonth_26', 303),\n",
    " ('DayofMonth_7', 301),\n",
    " ('DayofMonth_4', 300),\n",
    " ('DayofMonth_2', 299),\n",
    " ('DayofMonth_27', 298),\n",
    " ('DayofMonth_3', 294),\n",
    " ('DayofMonth_28', 287),\n",
    " ('Month_2', 284),\n",
    " ('DayofMonth_5', 284),\n",
    " ('DayofMonth_12', 282),\n",
    " ('DayofMonth_11', 276),\n",
    " ('Dest_CLT', 276),\n",
    " ('DayofMonth_6', 275),\n",
    " ('DayofMonth_22', 274),\n",
    " ('DayofMonth_8', 273),\n",
    " ('DayofMonth_18', 273),\n",
    " ('DayofMonth_29', 267),\n",
    " ('DayofMonth_23', 266),\n",
    " ('DayofMonth_14', 263),\n",
    " ('DayofMonth_30', 260),\n",
    " ('DayofMonth_10', 259),\n",
    " ('DayofMonth_16', 257),\n",
    " ('DayofMonth_24', 255),\n",
    " ('DayofMonth_13', 245),\n",
    " ('DayofMonth_25', 237),\n",
    " ('is_holiday_1', 231),\n",
    " ('DayofMonth_31', 164)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の特徴量の重要度に基づき、出発地と目的地の両方における DepHourofDay、Airwind、Temperature がフライトの遅延を判断するうえで重要な要因であることがわかります。\n",
    "\n",
    "これは、特徴量に関して人間が直感的に理解できるものと、モデルが実際に学習するものを評価するための 1 つの方法です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "モデルのトレーニングと評価のイテレーションを何回か行いました。ここで、もう少し時間があることを想定して、このプロジェクトについてまとめ、これまでに学んだこと、今後どのような手順を実行するのかについてよく考えます。以下のセルを使用して、これらの質問や他の関連する質問に回答してください。\n",
    "\n",
    "1.モデルのパフォーマンスはビジネス目標を満たしていますか? 満たしていない場合、チューニングにさらに多くの時間をかけられるとしたら、別にやってみたいことが何かありますか?\n",
    "2.データセット、特徴量、ハイパーパラメータを変更すると、モデルはどの程度改善しましたか? このプロジェクト全体で採用した手法のうち、モデルが最も改善されたと感じたのはどのような種類の手法ですか?\n",
    "3.このプロジェクト全体で直面した最大の課題は何でしたか?\n",
    "4.パイプラインのフェーズに関する質問のうち、理解できていないままのものがありますか?\n",
    "5.このプロジェクトで機械学習について学んだ最も重要なことは何ですか? (3 つ挙げてください)\n",
    "\n",
    "#### <span style=\"color: blue;\">プロジェクトプレゼンテーション: この質問に対する自分の回答の要約をプロジェクトプレゼンテーションに含めます。この時点でプロジェクトプレゼンテーションに使用するメモをすべてまとめ、クラスで結果を発表する準備をします。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
